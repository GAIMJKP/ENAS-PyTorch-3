{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TempAndTC(nn.Module):\n",
    "    \"\"\"Apply tanh constant and temperature to controller's logits\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(TempAndTC, self).__init__()\n",
    "        \n",
    "    def forward(self, logits, temperature=None, tanh_constant=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits: tensor\n",
    "            temperature: float\n",
    "            tanh_constant: float\n",
    "        \"\"\"\n",
    "        if temperature is not None:\n",
    "            logits /= temperature\n",
    "        if tanh_constant is not None:\n",
    "            logits = tanh_constant * torch.tanh(logits)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexSampler(nn.Module):\n",
    "    \"\"\"Sample index\"\"\"\n",
    "    \n",
    "    def __init__(self, lstm_size):\n",
    "        super(IndexSampler, self).__init__()\n",
    "        self.w_attn_2 = nn.Linear(lstm_size, lstm_size)\n",
    "        self.v_attn = nn.Linear(lstm_size, 1)\n",
    "        self.logits_proc = TempAndTC()\n",
    "        \n",
    "    def forward(self, h, query, temperature=None, tanh_constant=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h: tensor, [layer_num, batch_size, lstm_size]\n",
    "            query: tensor, [node_num, lstm_size]\n",
    "            temperature: float\n",
    "            tanh_constant: float\n",
    "            \n",
    "        Returns:\n",
    "            index: int\n",
    "            logits: tensor, [1, num_exitsing_nodes]\n",
    "        \"\"\"\n",
    "        logits = self.v_attn(torch.tanh(query + self.w_attn_2(h[-1])))\n",
    "        logits = self.logits_proc(logits.view(1, -1), temperature, tanh_constant)\n",
    "        \n",
    "        # sample index using logits\n",
    "        index = torch.multinomial(logits.exp(), 1).item()\n",
    "        \n",
    "        return index, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptSampler(nn.Module):\n",
    "    \"\"\"Sample operation\"\"\"\n",
    "    \n",
    "    def __init__(self, lstm_size, num_branches, use_bias=True):\n",
    "        super(OptSampler, self).__init__()\n",
    "        \n",
    "        self.w_soft = nn.Linear(lstm_size, num_branches)\n",
    "        self.b_soft = nn.Parameter(torch.tensor([[10.0]*2 + [0.]*(num_branches - 2)]))\n",
    "        self.logits_proc = TempAndTC()\n",
    "        if use_bias:\n",
    "            self.b_soft_no_learn = nn.Parameter(torch.tensor([[0.25]*2 + \n",
    "                                                              [-0.25]*(num_branches - 2)]))\n",
    "            \n",
    "        assert self.b_soft.shape == self.b_soft_no_learn.shape == (1, num_branches)\n",
    "        \n",
    "    def forward(self, h, temperature=None, tanh_constant=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h: tensor, [layer_num, batch_size, lstm_size]\n",
    "            temperature: float\n",
    "            tanh_constant: float\n",
    "            \n",
    "        Returns:\n",
    "            index: int\n",
    "            logits: tensor, [1, num_branches]\n",
    "        \"\"\"\n",
    "        logits = self.w_soft(h[-1]) + self.b_soft\n",
    "        logits = self.logits_proc(logits, temperature, tanh_constant) # [1, num_branches]\n",
    "        if self.b_soft_no_learn is not None:\n",
    "            logits += self.b_soft_no_learn\n",
    "        \n",
    "        # sample operation id\n",
    "        op_id = torch.multinomial(logits.exp(), 1).item()\n",
    "        \n",
    "        return op_id, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MicroController(nn.Module):\n",
    "    \"\"\"Implement the LSTM micro controller for convolutional cell archecture generation.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        \"\"\"Constructs MicroController.\n",
    "        \"\"\"\n",
    "        super(MicroController, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # LSTM, index embedding and operation embeddings\n",
    "        self.lstm = nn.LSTM(config.lstm_size, config.lstm_size, config.num_lstm_layer)\n",
    "        self.g_emb = nn.Parameter(torch.randn(1, 1, config.lstm_size, requires_grad=True))\n",
    "        self.w_emb = nn.Embedding(config.num_branches, config.lstm_size)\n",
    "        \n",
    "        self.w_attn_1 = nn.Linear(config.lstm_size, config.lstm_size)\n",
    "        \n",
    "        self.opt_sampler = OptSampler(config.lstm_size, config.num_branches)\n",
    "        self.index_sampler = IndexSampler(config.lstm_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, prev_h=None, prev_c=None, use_bias=False):\n",
    "        \"\"\"Search for a cell stracture.\n",
    "           \n",
    "        Args:\n",
    "            prev_h: None (normal cell) or the last output c of LSTM (reduction).\n",
    "            prev_c: None (normal cell) or the last output c of LSTM (reduction).\n",
    "            use_bias: if the no-learn bias is used.\n",
    "            \n",
    "        Returns:\n",
    "            prev_h, prev_c: the last output of LSTM\n",
    "            arc_seq: list of integers, [index_1, index_2, op_1, op_2] * node_num\n",
    "            logits: list with length equals to node number. Each element in the list is\n",
    "                also a list that contains 4 tensors with size [1, num_of_prev_nodes or\n",
    "                num_of_branches].\n",
    "        \"\"\"\n",
    "        arc_seq, logits = [], []\n",
    "        anchors, anchors_w = [], [] # anchors to save all generated nodes\n",
    "        \n",
    "        if prev_c is None and prev_h is None:\n",
    "            prev_h = torch.zeros(self.config.num_lstm_layer, 1, self.config.lstm_size)\n",
    "            prev_c = torch.zeros(self.config.num_lstm_layer, 1, self.config.lstm_size)\n",
    "        else:\n",
    "            assert prev_c is not None and prev_h is not None, \"Prev_c and prev_h mush both be None!\"\n",
    "        \n",
    "        # sample 2 inputs\n",
    "        h, c = self._sample_input_nodes(self.g_emb, prev_h, prev_c, anchors, anchors_w)\n",
    "        \n",
    "        # sample the rest B - 2 nodes\n",
    "        for i in range(self.config.node_num):\n",
    "            (h, c), node_logits = self._sample_node(self.g_emb, h, c, arc_seq, \n",
    "                                                    anchors, anchors_w)\n",
    "            assert len(anchors) == len(anchors_w) == i + 3\n",
    "            logits.append(node_logits)\n",
    "            \n",
    "        assert len(arc_seq) // 4 == len(logits)\n",
    "        return (h, c), arc_seq, logits\n",
    "    \n",
    "    \n",
    "    def _sample_input_nodes(self, inputs, h, c, anchors, anchors_w):\n",
    "        \"\"\"Generate the first two input nodes, which are the outpus of previous cells,\n",
    "           and save them to anchors.\n",
    "        \n",
    "        Args:\n",
    "            inputs: tensor, [1, batch_size, lstm_size]\n",
    "            h: tensor, [num_layers, batch_size, hidden_size]\n",
    "            c: tensor, [num_layers, batch_size, hidden_size]\n",
    "            anchors: list of tensor: [1, 1, lstm_size]\n",
    "            anchors_w: list of tensor: : [1, lstm_size]\n",
    "            \n",
    "        Returns:\n",
    "            h, c: the output of LSTM at 5th step\n",
    "        \"\"\"            \n",
    "        for i in range(2):\n",
    "            output, (h, c) = self.lstm(inputs, (h, c))\n",
    "            anchors.append(torch.unsqueeze(torch.zeros_like(h[-1]), dim=0))\n",
    "            anchors_w.append(self.w_attn_1(h[-1]))\n",
    "            \n",
    "        return h, c\n",
    "        \n",
    "        \n",
    "    def _sample_node(self, inputs, prev_h, prev_c, arc_seq, anchors, anchors_w):\n",
    "        \"\"\"Sample one node which has 2 indexs and 2 operations. \n",
    "           \n",
    "           Note that there are in total 5 steps for LSTM. The first two steps to sample \n",
    "           two indexs, the second 2 steps to sample two operations, and the least step \n",
    "           to add the node to anchors.\n",
    "        \n",
    "        Returns:\n",
    "            h, c: the output of LSTM at 5th step.\n",
    "            node_logits: list of 4 tensors with size [1, num_of_prev_nodes or\n",
    "                num_of_branches]\n",
    "        \"\"\"\n",
    "        node_logits = []\n",
    "        \n",
    "        # the first two steps: sample indexs\n",
    "        query = torch.cat(anchors_w, dim=0)\n",
    "        assert query.shape == (len(anchors_w), self.config.lstm_size)\n",
    "        \n",
    "        for i in range(2):            \n",
    "            (h, c), index, logits = self._sample_index(inputs, prev_h, prev_c, query)\n",
    "            prev_h, prev_c, inputs = h, c, anchors[index]\n",
    "            assert inputs.shape == (1, 1, self.config.lstm_size), \"Oops, LSTM input size seems wrong!\"\n",
    "            \n",
    "            arc_seq.append(index)\n",
    "            node_logits.append(logits)\n",
    "            \n",
    "        # the second two steps: sample operations\n",
    "        for i in range(2):\n",
    "            (h, c), op_id, logits = self._sample_opera(inputs, prev_h, prev_c)\n",
    "            prev_h, prev_c, inputs = h, c, self.w_emb(torch.LongTensor([[op_id]]))\n",
    "            assert inputs.shape == (1, 1, self.config.lstm_size), \"Oops, LSTM input size seems wrong!\"\n",
    "            \n",
    "            arc_seq.append(op_id)\n",
    "            node_logits.append(logits)\n",
    "        \n",
    "        # one more step: add the node to anchors\n",
    "        h, c = self._add_node_to_anchors(inputs, prev_h, prev_c, anchors, anchors_w)\n",
    "        \n",
    "        return (h, c), node_logits\n",
    "        \n",
    "        \n",
    "    def _sample_index(self, inputs, h, c, query):\n",
    "        \"\"\"Sample index: find out the input node.\n",
    "        \n",
    "        Returns:\n",
    "            (h, c): tensors, the hidden state of the top layer LSTM.\n",
    "            index: int\n",
    "            logits: tensor, [1, num_of_existing_nodes]\n",
    "        \"\"\"\n",
    "        # attention\n",
    "        output, (h, c) = self.lstm(inputs, (h, c))\n",
    "        index, logits = self.index_sampler(h, query, \n",
    "                                           self.config.temperature, \n",
    "                                           self.config.tanh_constant)\n",
    "        return (h, c), index, logits\n",
    "    \n",
    "    \n",
    "    def _sample_opera(self, inputs, h, c):\n",
    "        \"\"\"Sample operation.\n",
    "        \n",
    "        Args:\n",
    "            inputs: [1, 1, lstm_size]\n",
    "            h, c: previous hidden LSTM state.\n",
    "\n",
    "        Returns:\n",
    "            (h, c): tensors, the hidden state of the top layer LSTM.\n",
    "            op_id: int\n",
    "            logits: tensor, [1, num_of_branches]         \n",
    "        \"\"\"\n",
    "        output, (h, c) = self.lstm(inputs, (h, c))\n",
    "        op_id, logits = self.opt_sampler(h, \n",
    "                                         self.config.temperature, \n",
    "                                         self.config.tanh_constant)\n",
    "        return (h, c), op_id, logits\n",
    "\n",
    "    \n",
    "    def _add_node_to_anchors(self, inputs, h, c, anchors, anchors_w):\n",
    "        \"\"\"The final step of generating a node: one more forward step of LSTM and \n",
    "           add the hidden state to anchors.\n",
    "        \"\"\"\n",
    "        output, (h, c) = self.lstm(inputs, (h, c))\n",
    "        anchors.append(torch.unsqueeze(h[-1], dim=0))\n",
    "        anchors_w.append(self.w_attn_1(h[-1]))\n",
    "        \n",
    "        return (h, c)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    out_channels = 20\n",
    "    num_layers = 15\n",
    "    node_num = 6\n",
    "    class_num = 10\n",
    "    image_size = 32\n",
    "    use_aux_heads = True\n",
    "    \n",
    "    # controller\n",
    "    num_lstm_layer = 2\n",
    "    lstm_size = 50\n",
    "    num_branches = 5\n",
    "    temperature = None\n",
    "    tanh_constant = 1.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "controller = MicroController(config)\n",
    "(prev_c, prev_h), arc_seq, logits = controller()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 24\n",
      "[0, 0, 1, 1, 1, 1, 0, 0, 3, 2, 1, 0, 2, 1, 1, 0, 0, 0, 0, 2, 5, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Length: {}\".format(len(arc_seq)))\n",
    "print(arc_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "4\n",
      "tensor([[0.0041, 0.0063]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.0074, 0.0095]], grad_fn=<MulBackward0>)\n",
      "tensor([[ 1.3500,  1.3500, -0.1594, -0.2475, -0.2880]], grad_fn=<AddBackward0>)\n",
      "tensor([[3.8574, 3.8574, 0.8409, 0.7806, 0.7532]], grad_fn=<ExpBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(len(logits))\n",
    "print(len(logits[0]))\n",
    "print(logits[0][0])\n",
    "print(logits[0][1])\n",
    "print(logits[0][2])\n",
    "print(logits[0][3].exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceLoss(nn.Module):\n",
    "    \"\"\"Compute reinforce loss.\"\"\"\n",
    "    \n",
    "    def __init__(self, entropy_weight=None, bl_dec=0.999):\n",
    "        super(ReinforceLoss, self).__init__()\n",
    "        self.entropy_weight = entropy_weight\n",
    "        self.bl_dec = bl_dec\n",
    "        \n",
    "        self.baseline = 0.\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        \n",
    "    def forward(self, reward, logits, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            reward: float, valid accuracy of child model.\n",
    "            logits: list whose length equals to node number [node_num, 4]. Each element \n",
    "                in the list is a tensor with size: [1, num_of_prev_nodes or num_of_branches].\n",
    "            target: tensor, list of integers, [index_1, index_2, op_1, op_2] * (node number)\n",
    "        \"\"\"\n",
    "        log_prob, sample_entropy = self._get_log_prob_and_entropy(logits, target)\n",
    "        if self.entropy_weight is not None:\n",
    "            reward += self.entropy_weight * sample_entropy\n",
    "        \n",
    "        self.baseline -= (1.0 - self.bl_dec) * (self.baseline - reward)\n",
    "        loss = log_prob * (reward - self.baseline)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "        \n",
    "    def _get_log_prob_and_entropy(self, logits, target):\n",
    "        \"\"\"Iterate over all nodes and operations.\n",
    "        \"\"\"\n",
    "        node_num, sample_num = len(logits), len(logits[0])\n",
    "        assert sample_num == 4, \"Each node should have 4 samples.\"\n",
    "        \n",
    "        log_prob, entropy = 0., 0.\n",
    "        for i in range(node_num):\n",
    "            for j in range(sample_num):\n",
    "                sampled_id, logit = target[i * 4 + j], logits[i][j]\n",
    "                sampled_id = torch.unsqueeze(sampled_id, dim=0)\n",
    "                log_prob += self._get_cross_entropy_loss(logit, sampled_id)\n",
    "                entropy += self._get_entropy(logit)\n",
    "        \n",
    "        log_prob = log_prob / (node_num * sample_num)\n",
    "        entropy = entropy / (node_num * sample_num)\n",
    "        \n",
    "        return log_prob, entropy\n",
    "                \n",
    "        \n",
    "    def _get_cross_entropy_loss(self, logits, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits: [1, num_of_prev_nodes or num_of_branches]\n",
    "            index: integer\n",
    "        \"\"\"\n",
    "        return self.criterion(logits, index)\n",
    "    \n",
    "    \n",
    "    def _get_entropy(self, logits):\n",
    "        \"\"\"Compute entropy using sampled label as ground truth.\n",
    "        \n",
    "        Args:\n",
    "            logits: [1, num_of_prev_nodes or num_of_branches]\n",
    "        \"\"\"\n",
    "        logits = torch.squeeze(logits, dim=0)\n",
    "        entropy = self._softmax_cross_entropy_with_logits(logits, \n",
    "                      F.softmax(logits, dim=-1)).item()\n",
    "        return entropy\n",
    "    \n",
    "    \n",
    "    def _softmax_cross_entropy_with_logits(self, logits, labels):\n",
    "        \"\"\"Implement tensorflow \"tf.nn.softmax_cross_entropy_with_logits\"\n",
    "\n",
    "        Args:\n",
    "            logits: [batch_size, num_classes]\n",
    "            labels: [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        assert logits.shape == labels.shape, \"Logits and labels should have same shape!\"\n",
    "        loss = torch.sum(-labels * F.log_softmax(logits, dim=-1), -1)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ReinforceLoss()\n",
    "arc_seq = torch.tensor(arc_seq)\n",
    "loss = criterion(0.9, logits, arc_seq)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ControllerModel:\n",
    "    \"\"\"Implement controller model for controller training, validating and testing.\"\"\"\n",
    "    \n",
    "    def __init(self, config, device, write_summary=True):\n",
    "        \"\"\"Initialize the model.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.logger = config.logger\n",
    "        self.device = device       \n",
    "        \n",
    "        self.controller = self._build_model(config)\n",
    "        \n",
    "        \n",
    "    def _build_model(self, config):\n",
    "        \"\"\"Build controller model.\n",
    "        \"\"\"\n",
    "        return MicroController(config)\n",
    "    \n",
    "        \n",
    "    def _initialize_model(self):\n",
    "        \"\"\"Model initialization.\n",
    "        \"\"\"\n",
    "        for p in model.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.uniform_(p, a=-0.1, b=0.1)\n",
    "        return model\n",
    "    \n",
    "        \n",
    "    def _get_optimizer(self, config, model):\n",
    "        \"\"\"Create Optimizer for training.\n",
    "        \"\"\"\n",
    "        return torch.optim.Adam(model.parameters(), lr=0, \n",
    "                                betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "    def _get_criterion(self, config):\n",
    "        \"\"\"Loss function.\n",
    "        \"\"\"\n",
    "        return ReinforceLoss(entropy_weight=config.entropy_weight, \n",
    "                             bl_dec=config.bl_dec)\n",
    "        \n",
    "    def load_weights(self, path):\n",
    "        \"\"\"Load pre-trained weights.\n",
    "        \"\"\"\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "        \n",
    "        \n",
    "    def loss_batch(self, loss_func, reward, logits, target, optimizer=None):\n",
    "        \"\"\"Compute loss and update model weights on a batch of data.\n",
    "        \"\"\"\n",
    "        loss = loss_func(reward, logits, target)\n",
    "        \n",
    "        if optimizer is not None:\n",
    "            with torch.set_grad_enabled(True):\n",
    "                loss.backward() # compute gradients\n",
    "                optimizer.step() # update weights\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = torch.tensor([1, 0], dtype=torch.long)\n",
    "labels = torch.tensor([1, 1], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = torch.sum(pred_labels == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
